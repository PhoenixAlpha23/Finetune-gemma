{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 10180928,
          "sourceType": "datasetVersion",
          "datasetId": 6048296
        },
        {
          "sourceId": 85986,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 72246,
          "modelId": 78150
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "gemma-trial",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhoenixAlpha23/Finetune-gemma/blob/main/gemma_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "oUny7QLE-rKz"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "aryanrahultandon_multilingual_idioms_indian_path = kagglehub.dataset_download('aryanrahultandon/multilingual-idioms-indian')\n",
        "keras_gemma2_keras_gemma2_instruct_2b_en_1_path = kagglehub.model_download('keras/gemma2/Keras/gemma2_instruct_2b_en/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "7cV7Sx-P-rK9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning to train gemma pretrained model on idioms from various languages"
      ],
      "metadata": {
        "id": "-G04KeTA-rK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-12-12T13:57:20.886077Z",
          "iopub.execute_input": "2024-12-12T13:57:20.886417Z",
          "iopub.status.idle": "2024-12-12T13:57:21.222998Z",
          "shell.execute_reply.started": "2024-12-12T13:57:20.886386Z",
          "shell.execute_reply": "2024-12-12T13:57:21.22203Z"
        },
        "trusted": true,
        "id": "NNS7SwlY-rLD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data= pd.read_json('/kaggle/input/multilingual-idioms-indian/combined_idioms.json')\n",
        "data.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-12T13:57:37.189818Z",
          "iopub.execute_input": "2024-12-12T13:57:37.190193Z",
          "iopub.status.idle": "2024-12-12T13:57:37.210026Z",
          "shell.execute_reply.started": "2024-12-12T13:57:37.19016Z",
          "shell.execute_reply": "2024-12-12T13:57:37.209157Z"
        },
        "trusted": true,
        "id": "sR_x7-Ej-rLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets bitsandbytes accelerate peft"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T13:57:40.68316Z",
          "iopub.execute_input": "2024-12-12T13:57:40.683524Z",
          "iopub.status.idle": "2024-12-12T13:57:52.436837Z",
          "shell.execute_reply.started": "2024-12-12T13:57:40.683483Z",
          "shell.execute_reply": "2024-12-12T13:57:52.435973Z"
        },
        "id": "MxANbmpy-rLG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "4Z67iMxW-rLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset from the Kaggle dataset path\n",
        "data = load_dataset('json', data_files='/kaggle/input/multilingual-idioms-indian/combined_idioms.json')\n",
        "\n",
        "# Preprocess dataset\n",
        "data = data['train'].map(lambda x: {\"input_text\": x[\"idiom\"], \"target_text\": x[\"figurative_meaning\"]})\n",
        "data = data.rename_column(\"input_text\", \"input_ids\")\n",
        "data = data.rename_column(\"target_text\", \"labels\")\n",
        "data = data.train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T13:58:01.763033Z",
          "iopub.execute_input": "2024-12-12T13:58:01.763379Z",
          "iopub.status.idle": "2024-12-12T13:58:03.036518Z",
          "shell.execute_reply.started": "2024-12-12T13:58:01.76335Z",
          "shell.execute_reply": "2024-12-12T13:58:03.035827Z"
        },
        "id": "hkq0kClP-rLI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q -U keras-nlp datasets\n",
        "!pip install -q -U keras"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T13:58:10.232222Z",
          "iopub.execute_input": "2024-12-12T13:58:10.23297Z",
          "iopub.status.idle": "2024-12-12T13:58:31.701423Z",
          "shell.execute_reply.started": "2024-12-12T13:58:10.232936Z",
          "shell.execute_reply": "2024-12-12T13:58:31.70023Z"
        },
        "id": "GlnlNfHW-rLK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the backbend before importing Keras\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "# Avoid memory fragmentation on JAX backend.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "\n",
        "# Run at half precision.\n",
        "#keras.config.set_floatx(\"bfloat16\")\n",
        "\n",
        "# Training Configurations\n",
        "token_limit = 256\n",
        "num_data_limit = 100\n",
        "lora_name = \"cakeboss\"\n",
        "lora_rank = 4\n",
        "lr_value = 1e-4\n",
        "train_epoch = 20\n",
        "model_id = \"gemma2_instruct_2b_en\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T14:01:47.982262Z",
          "iopub.execute_input": "2024-12-12T14:01:47.982657Z",
          "iopub.status.idle": "2024-12-12T14:01:47.987725Z",
          "shell.execute_reply.started": "2024-12-12T14:01:47.982626Z",
          "shell.execute_reply": "2024-12-12T14:01:47.986844Z"
        },
        "id": "b-aJyQWz-rLL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras_nlp\n",
        "\n",
        "import time\n",
        "\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\n",
        "gemma_lm.summary()\n",
        "\n",
        "tick_start = 0\n",
        "\n",
        "def tick():\n",
        "    global tick_start\n",
        "    tick_start = time.time()\n",
        "\n",
        "def tock():\n",
        "    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n",
        "\n",
        "def text_gen(prompt):\n",
        "    tick()\n",
        "    input = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    output = gemma_lm.generate(input, max_length=token_limit)\n",
        "    print(\"\\nGemma output:\")\n",
        "    print(output)\n",
        "    tock()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T14:01:51.324694Z",
          "iopub.execute_input": "2024-12-12T14:01:51.325574Z",
          "iopub.status.idle": "2024-12-12T14:02:35.842553Z",
          "shell.execute_reply.started": "2024-12-12T14:01:51.32554Z",
          "shell.execute_reply": "2024-12-12T14:02:35.841753Z"
        },
        "id": "dh9vjkdJ-rLN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"gemma_causal_lm_preprocessor\"  # Replace with GEMMA 2B model path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T14:06:28.473973Z",
          "iopub.execute_input": "2024-12-12T14:06:28.474363Z",
          "iopub.status.idle": "2024-12-12T14:06:28.737837Z",
          "shell.execute_reply.started": "2024-12-12T14:06:28.474333Z",
          "shell.execute_reply": "2024-12-12T14:06:28.736291Z"
        },
        "id": "WfC8el6m-rLO"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}