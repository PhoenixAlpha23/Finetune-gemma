{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":10267573,"datasetId":6048296,"databundleVersionId":10566819},{"sourceType":"modelInstanceVersion","sourceId":85984,"databundleVersionId":9247149,"modelInstanceId":72244}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_json)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:28:33.928062Z","iopub.execute_input":"2024-12-22T10:28:33.928400Z","iopub.status.idle":"2024-12-22T10:28:34.225289Z","shell.execute_reply.started":"2024-12-22T10:28:33.928369Z","shell.execute_reply":"2024-12-22T10:28:34.224391Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/multilingual-idioms-indian/malayalam.json\n/kaggle/input/multilingual-idioms-indian/hindi.json\n/kaggle/input/multilingual-idioms-indian/punjabi.json\n/kaggle/input/multilingual-idioms-indian/gujrati.json\n/kaggle/input/multilingual-idioms-indian/marathi.json\n/kaggle/input/multilingual-idioms-indian/kannada.json\n/kaggle/input/multilingual-idioms-indian/telugu.json\n/kaggle/input/multilingual-idioms-indian/balanced_combined_idioms.json\n/kaggle/input/multilingual-idioms-indian/english.json\n/kaggle/input/multilingual-idioms-indian/urdu.json\n/kaggle/input/gemma2/keras/gemma2_2b_en/1/config.json\n/kaggle/input/gemma2/keras/gemma2_2b_en/1/tokenizer.json\n/kaggle/input/gemma2/keras/gemma2_2b_en/1/metadata.json\n/kaggle/input/gemma2/keras/gemma2_2b_en/1/model.weights.h5\n/kaggle/input/gemma2/keras/gemma2_2b_en/1/assets/tokenizer/vocabulary.spm\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"data= pd.read_json('/kaggle/input/multilingual-idioms-indian/balanced_combined_idioms.json')\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:28:34.226551Z","iopub.execute_input":"2024-12-22T10:28:34.226998Z","iopub.status.idle":"2024-12-22T10:28:34.261116Z","shell.execute_reply.started":"2024-12-22T10:28:34.226963Z","shell.execute_reply":"2024-12-22T10:28:34.260371Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                          idiom                        literal_meaning  \\\n0            Make two ends meet                  To connect both ends.   \n1                   Break a leg                     To fracture a leg.   \n2        Through thick and thin     In both thick and thin conditions.   \n3    Let the cat out of the bag           To release a cat from a bag.   \n4          Elephant in the room  A large elephant present in the room.   \n..                          ...                                    ...   \n295          بندر کیا جانے ادرک     What does a monkey know of ginger?   \n296     چپ رہنے میں ہی عافیت ہے          'There is safety in silence.'   \n297                بندہ باندھیے                        Tied up person.   \n298     چپ رہنے میں ہی عافیت ہے          'There is safety in silence.'   \n299     چپ رہنے میں ہی عافیت ہے          'There is safety in silence.'   \n\n                                    figurative_meaning  \\\n0                       Struggling to manage finances.   \n1    Wishing someone success, especially before a p...   \n2     Loyalty and support regardless of circumstances.   \n3                       Revealing a secret carelessly.   \n4        A major problem that people avoid discussing.   \n..                                                 ...   \n295  Used to indicate that someone does not underst...   \n296  'Sometimes it is better to remain silent than ...   \n297  Someone who is dependent on others for help or...   \n298  'Sometimes it is better to remain silent than ...   \n299  'Sometimes it is better to remain silent than ...   \n\n                                               example language  \n0    After losing his job, he found it hard to make...  English  \n1    'Break a leg' before your performance tonight;...  English  \n2    They have been friends through thick and thin ...  English  \n3    She let the cat out of the bag about the surpr...  English  \n4    'We need to address the elephant in the room r...  English  \n..                                                 ...      ...  \n295  'وہ اس کتاب کی اہمیت کو نہیں سمجھتا، بندر کیا ...     Urdu  \n296         'بہت سی باتوں پر چپ رہنے میں ہی عافیت ہے۔'     Urdu  \n297  'وہ ہمیشہ دوسرے لوگوں پر منحصر رہتا ہے، واقعی ...     Urdu  \n298         'بہت سی باتوں پر چپ رہنے میں ہی عافیت ہے۔'     Urdu  \n299         'بہت سی باتوں پر چپ رہنے میں ہی عافیت ہے۔'     Urdu  \n\n[300 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idiom</th>\n      <th>literal_meaning</th>\n      <th>figurative_meaning</th>\n      <th>example</th>\n      <th>language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Make two ends meet</td>\n      <td>To connect both ends.</td>\n      <td>Struggling to manage finances.</td>\n      <td>After losing his job, he found it hard to make...</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Break a leg</td>\n      <td>To fracture a leg.</td>\n      <td>Wishing someone success, especially before a p...</td>\n      <td>'Break a leg' before your performance tonight;...</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Through thick and thin</td>\n      <td>In both thick and thin conditions.</td>\n      <td>Loyalty and support regardless of circumstances.</td>\n      <td>They have been friends through thick and thin ...</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Let the cat out of the bag</td>\n      <td>To release a cat from a bag.</td>\n      <td>Revealing a secret carelessly.</td>\n      <td>She let the cat out of the bag about the surpr...</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Elephant in the room</td>\n      <td>A large elephant present in the room.</td>\n      <td>A major problem that people avoid discussing.</td>\n      <td>'We need to address the elephant in the room r...</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>بندر کیا جانے ادرک</td>\n      <td>What does a monkey know of ginger?</td>\n      <td>Used to indicate that someone does not underst...</td>\n      <td>'وہ اس کتاب کی اہمیت کو نہیں سمجھتا، بندر کیا ...</td>\n      <td>Urdu</td>\n    </tr>\n    <tr>\n      <th>296</th>\n      <td>چپ رہنے میں ہی عافیت ہے</td>\n      <td>'There is safety in silence.'</td>\n      <td>'Sometimes it is better to remain silent than ...</td>\n      <td>'بہت سی باتوں پر چپ رہنے میں ہی عافیت ہے۔'</td>\n      <td>Urdu</td>\n    </tr>\n    <tr>\n      <th>297</th>\n      <td>بندہ باندھیے</td>\n      <td>Tied up person.</td>\n      <td>Someone who is dependent on others for help or...</td>\n      <td>'وہ ہمیشہ دوسرے لوگوں پر منحصر رہتا ہے، واقعی ...</td>\n      <td>Urdu</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>چپ رہنے میں ہی عافیت ہے</td>\n      <td>'There is safety in silence.'</td>\n      <td>'Sometimes it is better to remain silent than ...</td>\n      <td>'بہت سی باتوں پر چپ رہنے میں ہی عافیت ہے۔'</td>\n      <td>Urdu</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>چپ رہنے میں ہی عافیت ہے</td>\n      <td>'There is safety in silence.'</td>\n      <td>'Sometimes it is better to remain silent than ...</td>\n      <td>'بہت سی باتوں پر چپ رہنے میں ہی عافیت ہے۔'</td>\n      <td>Urdu</td>\n    </tr>\n  </tbody>\n</table>\n<p>300 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"!pip install -q -U keras-nlp\n!pip install -q -U keras>=3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:28:34.262300Z","iopub.execute_input":"2024-12-22T10:28:34.262505Z","iopub.status.idle":"2024-12-22T10:28:44.265505Z","shell.execute_reply.started":"2024-12-22T10:28:34.262488Z","shell.execute_reply":"2024-12-22T10:28:44.264620Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\n\nos.environ['KERAS_BACKEND'] = 'jax'\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:28:44.266789Z","iopub.execute_input":"2024-12-22T10:28:44.267044Z","iopub.status.idle":"2024-12-22T10:28:44.271296Z","shell.execute_reply.started":"2024-12-22T10:28:44.267024Z","shell.execute_reply":"2024-12-22T10:28:44.270389Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import keras\nimport keras_nlp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:28:44.272250Z","iopub.execute_input":"2024-12-22T10:28:44.272515Z","iopub.status.idle":"2024-12-22T10:28:53.148580Z","shell.execute_reply.started":"2024-12-22T10:28:44.272494Z","shell.execute_reply":"2024-12-22T10:28:53.147675Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"json\",data_files='/kaggle/input/multilingual-idioms-indian/balanced_combined_idioms.json')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:28:53.149476Z","iopub.execute_input":"2024-12-22T10:28:53.150065Z","iopub.status.idle":"2024-12-22T10:28:54.629859Z","shell.execute_reply.started":"2024-12-22T10:28:53.150034Z","shell.execute_reply":"2024-12-22T10:28:54.628991Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e5936218ec0492b8cdbe8e7403f4d0d"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# First, modify the training data format to encourage translation and explanation\ndata = []\ntrain_data=ds['train']\nfor example in train_data:\n    if not all(key in example for key in [\"idiom\", \"literal_meaning\", \"figurative_meaning\", \"example\", \"language\"]):\n        continue\n        \n    template = (\n        \"Instruction:\\n\"\n        \"Find a suitable idiom for this situation: {figurative_meaning}\\n\\n\"\n        \"Response:\\n\"\n        \"Original Idiom ({language}): {idiom}\\n\"\n        \"English Translation: {literal_meaning}\\n\"\n        \"Example Usage: {example}\\n\"\n        \"Cultural Context: This idiom comes from {language} and is commonly used when {figurative_meaning}.\\n\"\n    )\n    \n    # If the idiom is already in English, use it directly\n    if example[\"language\"].lower() == \"english\":\n        example[\"english_translation\"] = example[\"idiom\"]\n    else:\n        # For non-English idioms, we need to ensure there's a translation\n        # You might need to add this to your dataset if not present\n        example[\"english_translation\"] = f\"[Translation: {example.get('english_translation', 'needs translation')}]\"\n    \n    data.append(template.format(**example))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:36:02.057966Z","iopub.execute_input":"2024-12-22T10:36:02.058300Z","iopub.status.idle":"2024-12-22T10:36:02.080955Z","shell.execute_reply.started":"2024-12-22T10:36:02.058277Z","shell.execute_reply":"2024-12-22T10:36:02.080220Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from datasets import DatasetDict\n# Initialize an empty list to store the formatted examples\ndata = []\n# Access the 'train' split of your dataset\ntrain_data = ds[\"train\"]\n\n# Add debug printing to see what fields are available\nprint(\"First example keys:\", list(train_data[0].keys()))\n\n# Iterate over each example in the dataset\nfor i, example in enumerate(train_data):\n    try:\n        # Check if required fields are available and valid\n        required_fields = [\"idiom\", \"literal_meaning\", \"figurative_meaning\", \"example\"]\n        \n        # Print missing fields for debugging\n        missing_fields = [field for field in required_fields if field not in example]\n        if missing_fields:\n            print(f\"Example {i} is missing fields: {missing_fields}\")\n            continue\n            \n        # Create a template with instruction and response format\n        template = (\n            \"Instruction:\\n Which idiom would be suitable for {figurative_meaning}\\n\\n\"\n            \"Response:\\n\"\n            \"Idiom: {idiom}\\n\\n\"\n            \"Example Usage: {example}\"\n        )\n        \n        # Format the example and add it to the data list\n        formatted_example = template.format(**example)\n        data.append(formatted_example)\n        \n    except KeyError as e:\n        print(f\"KeyError in example {i}: {str(e)}\")\n        print(f\"Available keys: {list(example.keys())}\")\n        continue\n\n# Limit to the first 1000 examples\ndata = data[:1000]\n\n# Display the first few formatted examples\nfor i, example in enumerate(data[:5]):\n    print(f\"Example {i + 1}:\\n{example}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")\ngemma_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:30:49.415348Z","iopub.execute_input":"2024-12-22T10:30:49.415692Z","iopub.status.idle":"2024-12-22T10:31:37.481833Z","shell.execute_reply.started":"2024-12-22T10:30:49.415666Z","shell.execute_reply":"2024-12-22T10:31:37.481120Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 4.\ngemma_lm.backbone.enable_lora(rank=4)\ngemma_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:31:45.498114Z","iopub.execute_input":"2024-12-22T10:31:45.498419Z","iopub.status.idle":"2024-12-22T10:31:45.831078Z","shell.execute_reply.started":"2024-12-22T10:31:45.498395Z","shell.execute_reply":"2024-12-22T10:31:45.830412Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,617,270,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Limit the input sequence length to 256 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = 256\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\ngemma_lm.fit(data, epochs=1, batch_size=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:31:55.864219Z","iopub.execute_input":"2024-12-22T10:31:55.864510Z","iopub.status.idle":"2024-12-22T10:34:44.960733Z","shell.execute_reply.started":"2024-12-22T10:31:55.864487Z","shell.execute_reply":"2024-12-22T10:34:44.960000Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 473ms/step - loss: 0.9032 - sparse_categorical_accuracy: 0.5776\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7e4d2772c9d0>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Test inference\ntest_situation = \"someone who is very nervous before an important event\"\nprompt = inference_template.format(test_situation)\n\n# Adjust sampling parameters to encourage creativity\nsampler = keras_nlp.samplers.TopKSampler(k=10, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=512))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:36:12.225857Z","iopub.execute_input":"2024-12-22T10:36:12.226157Z","iopub.status.idle":"2024-12-22T10:36:30.171966Z","shell.execute_reply.started":"2024-12-22T10:36:12.226135Z","shell.execute_reply":"2024-12-22T10:36:30.171101Z"}},"outputs":[{"name":"stdout","text":"\nInstruction:\nFind a suitable idiom for this situation: someone who is very nervous before an important event\n\nPlease provide:\n1. An appropriate idiom (from any language)\n2. Its English translation if not in English\n3. A clear example of usage\n4. Brief cultural context\n\nResponse:\nOriginal Idiom: കാശി പെട്ടി വളയ്ക്കു.\nEnglish Translation: To go into convulsions.\nCultural Context: Malayalam.\nExample Usage: 'ഒരു കാര്യത്തിന് ആവശ്യമായ കാശി പെട്ടി വളയ്ക്കുന്നവർ ഇല്ല എങ്കിൽ മതിയായി.'\nBrief Cultural Context: The idiom comes from Malayalam and is used when someone is very nervous before an important event.\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"#Define figurative meaning to be tested\ntest_meaning = \"Someone who is carefree  \"\n\n\n#Using the same template format as training\nprompt = (\n    \"Instruction:\\n Which  idiom would be suitable for {}\\n\\n\"\n    \"Response:\\n\"\n).format(test_meaning)\n\n\nsampler = keras_nlp.samplers.TopKSampler(k=10, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=512))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}