# Finetune-gemma

If anyone is reading this, im lost here mate,I really need guidance.

As I learn more about finetuning , LORA and QLORA, I realise im very interested in it.

Goal for myself:
-- 1st fine-tuning project post on linkedin before this week ends


What ive learned so far is that to use fine-tuning we first need to Quantize the llm, using techniques like LORA and QLORA.
Lower Order Rank Adaptation, because one cannot use the llm in its full capacity on personal systems with extremely limited processing lower.

It is a natural trade off between Accuracy and usability, one we are more than ahappy to make.

